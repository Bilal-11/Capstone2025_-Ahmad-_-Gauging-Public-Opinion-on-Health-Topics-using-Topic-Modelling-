{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Convention:** Code cells using referenced code begin with a comment `# REFERRED`. Code cells containing code I wrote begin with a comment `# MY CODE`. In cells that have both referenced and original code, the respective code parts are labelled.\n"
      ],
      "metadata": {
        "id": "tDsrvSmJ370p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load DATASET"
      ],
      "metadata": {
        "id": "tfQxxGjQ61_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJHXCSIq636p",
        "outputId": "19b1b9c0-c795-48ef-a4df-a5e607bab119"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MY CODE\n",
        "import os\n",
        "import json\n",
        "\n",
        "dataset_dir = '/content/drive/MyDrive/DATASET'\n",
        "files = []\n",
        "for (root,dirs,file) in os.walk(dataset_dir):\n",
        "  files = file\n",
        "\n",
        "json_files = []\n",
        "text_files = []\n",
        "\n",
        "for file in files:\n",
        "  if '.json' in file:\n",
        "    json_files.append(file)\n",
        "  else:\n",
        "    text_files.append(file)\n",
        "\n",
        "print('json files:', len(json_files))\n",
        "print('text files:', len(text_files))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpf0FJ4N8P3V",
        "outputId": "3abd8f9f-424f-47ae-a503-82d5c348f188"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "json files: 48\n",
            "text files: 48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing"
      ],
      "metadata": {
        "id": "MOTVB2Z454EU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IrxINd-j3hFO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Reference: https://www.kaggle.com/code/rockystats/topic-modelling-using-nmf*"
      ],
      "metadata": {
        "id": "4FQFMM73DvQN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fd348cf1"
      },
      "outputs": [],
      "source": [
        "# REFERRED\n",
        "\n",
        "## defining all utilty functions - needed for Data cleaning and processing\n",
        "\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "import string\n",
        "import re\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import TweetTokenizer, RegexpTokenizer\n",
        "import nltk\n",
        "\n",
        "# Contraction map\n",
        "c_dict = {\n",
        "    \"ain't\": \"am not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"can't've\": \"cannot have\",\n",
        "    \"'cause\": \"because\",\n",
        "    \"could've\": \"could have\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"couldn't've\": \"could not have\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"hadn't've\": \"had not have\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"he'd\": \"he would\",\n",
        "    \"he'd've\": \"he would have\",\n",
        "    \"he'll\": \"he will\",\n",
        "    \"he'll've\": \"he will have\",\n",
        "    \"he's\": \"he is\",\n",
        "    \"how'd\": \"how did\",\n",
        "    \"how'd'y\": \"how do you\",\n",
        "    \"how'll\": \"how will\",\n",
        "    \"how's\": \"how is\",\n",
        "    \"i'd\": \"I would\",\n",
        "    \"i'd've\": \"I would have\",\n",
        "    \"i'll\": \"I will\",\n",
        "    \"i'll've\": \"I will have\",\n",
        "    \"i'm\": \"I am\",\n",
        "    \"i've\": \"I have\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"it'd\": \"it had\",\n",
        "    \"it'd've\": \"it would have\",\n",
        "    \"it'll\": \"it will\",\n",
        "    \"it'll've\": \"it will have\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"let's\": \"let us\",\n",
        "    \"ma'am\": \"madam\",\n",
        "    \"mayn't\": \"may not\",\n",
        "    \"might've\": \"might have\",\n",
        "    \"mightn't\": \"might not\",\n",
        "    \"mightn't've\": \"might not have\",\n",
        "    \"must've\": \"must have\",\n",
        "    \"mustn't\": \"must not\",\n",
        "    \"mustn't've\": \"must not have\",\n",
        "    \"needn't\": \"need not\",\n",
        "    \"needn't've\": \"need not have\",\n",
        "    \"o'clock\": \"of the clock\",\n",
        "    \"oughtn't\": \"ought not\",\n",
        "    \"oughtn't've\": \"ought not have\",\n",
        "    \"shan't\": \"shall not\",\n",
        "    \"sha'n't\": \"shall not\",\n",
        "    \"shan't've\": \"shall not have\",\n",
        "    \"she'd\": \"she would\",\n",
        "    \"she'd've\": \"she would have\",\n",
        "    \"she'll\": \"she will\",\n",
        "    \"she'll've\": \"she will have\",\n",
        "    \"she's\": \"she is\",\n",
        "    \"should've\": \"should have\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"shouldn't've\": \"should not have\",\n",
        "    \"so've\": \"so have\",\n",
        "    \"so's\": \"so is\",\n",
        "    \"that'd\": \"that would\",\n",
        "    \"that'd've\": \"that would have\",\n",
        "    \"that's\": \"that is\",\n",
        "    \"there'd\": \"there had\",\n",
        "    \"there'd've\": \"there would have\",\n",
        "    \"there's\": \"there is\",\n",
        "    \"they'd\": \"they would\",\n",
        "    \"they'd've\": \"they would have\",\n",
        "    \"they'll\": \"they will\",\n",
        "    \"they'll've\": \"they will have\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"they've\": \"they have\",\n",
        "    \"to've\": \"to have\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"we'd\": \"we had\",\n",
        "    \"we'd've\": \"we would have\",\n",
        "    \"we'll\": \"we will\",\n",
        "    \"we'll've\": \"we will have\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"we've\": \"we have\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"what'll\": \"what will\",\n",
        "    \"what'll've\": \"what will have\",\n",
        "    \"what're\": \"what are\",\n",
        "    \"what's\": \"what is\",\n",
        "    \"what've\": \"what have\",\n",
        "    \"when's\": \"when is\",\n",
        "    \"when've\": \"when have\",\n",
        "    \"where'd\": \"where did\",\n",
        "    \"where's\": \"where is\",\n",
        "    \"where've\": \"where have\",\n",
        "    \"who'll\": \"who will\",\n",
        "    \"who'll've\": \"who will have\",\n",
        "    \"who's\": \"who is\",\n",
        "    \"who've\": \"who have\",\n",
        "    \"why's\": \"why is\",\n",
        "    \"why've\": \"why have\",\n",
        "    \"will've\": \"will have\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"won't've\": \"will not have\",\n",
        "    \"would've\": \"would have\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"wouldn't've\": \"would not have\",\n",
        "    \"y'all\": \"you all\",\n",
        "    \"y'alls\": \"you alls\",\n",
        "    \"y'all'd\": \"you all would\",\n",
        "    \"y'all'd've\": \"you all would have\",\n",
        "    \"y'all're\": \"you all are\",\n",
        "    \"y'all've\": \"you all have\",\n",
        "    \"you'd\": \"you had\",\n",
        "    \"you'd've\": \"you would have\",\n",
        "    \"you'll\": \"you you will\",\n",
        "    \"you'll've\": \"you you will have\",\n",
        "    \"you're\": \"you are\",\n",
        "    \"you've\": \"you have\"\n",
        "}\n",
        "\n",
        "# Compiling the contraction dict\n",
        "c_re = re.compile('(%s)' % '|'.join(c_dict.keys()))\n",
        "\n",
        "# List of stop words\n",
        "add_stop = ['said', 'say', '...', 'like', 'cnn', 'ad']\n",
        "stop_words = ENGLISH_STOP_WORDS.union(add_stop)\n",
        "\n",
        "# List of punctuation\n",
        "punc = list(set(string.punctuation))\n",
        "\n",
        "\n",
        "# Splits words on white spaces (leaves contractions intact) and splits out\n",
        "# trailing punctuation\n",
        "def casual_tokenizer(text):\n",
        "    tokenizer = TweetTokenizer()\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def expandContractions(text, c_re=c_re):\n",
        "    def replace(match):\n",
        "        return c_dict[match.group(0)]\n",
        "    return c_re.sub(replace, text)\n",
        "\n",
        "\n",
        "def process_text(text):\n",
        "    text = casual_tokenizer(text)\n",
        "    text = [each.lower() for each in text]\n",
        "    text = [re.sub('[0-9]+', '', each) for each in text]\n",
        "    text = [expandContractions(each, c_re=c_re) for each in text]\n",
        "    text = [SnowballStemmer('english').stem(each) for each in text]\n",
        "    text = [w for w in text if w not in punc]\n",
        "    text = [w for w in text if w not in stop_words]\n",
        "    text = [each for each in text if len(each) > 1]\n",
        "    text = [each for each in text if ' ' not in each]\n",
        "    return text\n",
        "\n",
        "\n",
        "def top_words(topic, n_top_words):\n",
        "    return topic.argsort()[:-n_top_words - 1:-1]\n",
        "\n",
        "\n",
        "def topic_table(model, feature_names, n_top_words):\n",
        "    topics = {}\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        t = (topic_idx)\n",
        "        topics[t] = [feature_names[i] for i in top_words(topic, n_top_words)]\n",
        "    return pd.DataFrame(topics)\n",
        "\n",
        "\n",
        "def whitespace_tokenizer(text):\n",
        "    pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
        "    tokenizer_regex = RegexpTokenizer(pattern)\n",
        "    tokens = tokenizer_regex.tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "# Funtion to remove duplicate words\n",
        "def unique_words(text):\n",
        "    ulist = []\n",
        "    [ulist.append(x) for x in text if x not in ulist]\n",
        "    return ulist\n",
        "\n",
        "\n",
        "def word_count(text):\n",
        "    return len(str(text).split(' '))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# REFERRED\n",
        "\n",
        "# Removing stemming step as it is not required for evaluating BERTopic\n",
        "\n",
        "def process_text2(text):\n",
        "    text = casual_tokenizer(text)\n",
        "    text = [each.lower() for each in text]\n",
        "    text = [re.sub('[0-9]+', '', each) for each in text]\n",
        "    text = [expandContractions(each, c_re=c_re) for each in text]\n",
        "    # text = [SnowballStemmer('english').stem(each) for each in text]\n",
        "    text = [w for w in text if w not in punc]\n",
        "    text = [w for w in text if w not in stop_words]\n",
        "    text = [each for each in text if len(each) > 1]\n",
        "    text = [each for each in text if ' ' not in each]\n",
        "    return text"
      ],
      "metadata": {
        "id": "XPfOOu_Om5Wt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# REFERRED\n",
        "\n",
        "# Removing punctuations for evaluating noun phrase approach\n",
        "\n",
        "def process_text3(text):\n",
        "    text = casual_tokenizer(text)\n",
        "    # text = [each.lower() for each in text]\n",
        "    # text = [re.sub('[0-9]+', '', each) for each in text]\n",
        "    # text = [expandContractions(each, c_re=c_re) for each in text]\n",
        "    # text = [SnowballStemmer('english').stem(each) for each in text]\n",
        "    text = [w for w in text if w not in punc]\n",
        "    # text = [w for w in text if w not in stop_words]\n",
        "    text = [each for each in text if len(each) > 1]\n",
        "    text = [each for each in text if ' ' not in each]\n",
        "    return text"
      ],
      "metadata": {
        "id": "-m7zdcsRgeqJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Noun-Phrase Approach"
      ],
      "metadata": {
        "id": "j5JQnDKLAL3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# MY CODE\n",
        "def get_noun_phrases(text):\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  doc = nlp(text)\n",
        "  noun_phrases = [chunk.text for chunk in doc.noun_chunks if chunk.root.pos_ != 'PRON']\n",
        "  return noun_phrases"
      ],
      "metadata": {
        "id": "MrStIk0y52rp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tm_noun_phase(filename):\n",
        "  # MY CODE\n",
        "  with open(dataset_dir+'/'+filename,'r') as dataset:\n",
        "    data = json.load(dataset)\n",
        "\n",
        "  comments = data['comments']\n",
        "\n",
        "  topic_list_noun_phrase = []\n",
        "  for comment in comments:\n",
        "    noun_phrases = get_noun_phrases(comment)\n",
        "    topic_list_noun_phrase.append(' '.join(noun_phrases))\n",
        "\n",
        "  with open(f'[NP_TOPICS_LIST]{filename}','w',encoding='utf-8') as output_file:\n",
        "    output_file.write(json.dumps(topic_list_noun_phrase,indent=4))"
      ],
      "metadata": {
        "id": "xk5iP0uW2dp9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MY CODE\n",
        "# for file in json_files:\n",
        "#   try:\n",
        "#     tm_noun_phase(file)\n",
        "#   except:\n",
        "#     print(file)\n",
        "#     continue"
      ],
      "metadata": {
        "id": "VDoG6Qq97tHZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "KpJCWGTN0o0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MY CODE\n",
        "from itertools import combinations\n",
        "import math\n",
        "\n",
        "def npmi_from_bigrams(topic,all_bigrams,method='sum'):\n",
        "  total_bigram_count = len(all_bigrams)\n",
        "\n",
        "  topic_words = topic.split(' ')\n",
        "  topic_pairs = list(combinations(topic_words,2))\n",
        "\n",
        "  npmi_list = []\n",
        "  for (w1,w2) in topic_pairs:\n",
        "    joint_count_w1_w2 = len([(a,b) for (a,b) in all_bigrams if a == w1 and b == w2])\n",
        "    joint_count_w2_w1 = len([(a,b) for (a,b) in all_bigrams if b == w1 and a == w2])\n",
        "    p_i_j = (joint_count_w1_w2/total_bigram_count) + (joint_count_w2_w1/total_bigram_count)\n",
        "    p_i_star = len([(a,b) for (a,b) in all_bigrams if a == w1])/total_bigram_count\n",
        "    p_star_j = len([(a,b) for (a,b) in all_bigrams if b == w2])/total_bigram_count\n",
        "\n",
        "    if p_i_j == 0 or p_i_star == 0 or p_star_j == 0:\n",
        "      pmi_w1_w2 = 0\n",
        "      npmi_w1_w2 = 0\n",
        "      npmi_list.append(npmi_w1_w2)\n",
        "      continue\n",
        "\n",
        "    pmi_w1_w2 = math.log(p_i_j/(p_i_star*p_star_j))\n",
        "\n",
        "    npmi_w1_w2 = pmi_w1_w2/-math.log(p_i_j)\n",
        "    npmi_list.append(npmi_w1_w2)\n",
        "\n",
        "  if method == 'sum':\n",
        "    return sum(npmi_list)\n",
        "  elif method == 'avg':\n",
        "    return sum(npmi_list)/len(npmi_list)\n",
        "  else:\n",
        "    raise Exception(\"method can only be 'sum' or 'avg'\")"
      ],
      "metadata": {
        "id": "EgxFQrig2ajv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MY CODE\n",
        "import json\n",
        "import time\n",
        "from nltk import bigrams\n",
        "\n",
        "def eval_noun_phrase(text_filename):\n",
        "  np_topics_dir = '/content/drive/MyDrive/NP_TOPICS'\n",
        "  with open(np_topics_dir+'/[NP_TOPICS_LIST]'+text_filename.replace('txt','json'),'r') as dataset:\n",
        "    topics = json.load(dataset)\n",
        "\n",
        "  article = ''\n",
        "  with open(dataset_dir+'/'+text_filename,'r') as file:\n",
        "    article = file.read()\n",
        "\n",
        "  with open(dataset_dir+'/'+text_filename.replace('txt','json'),'r') as dataset:\n",
        "    data = json.load(dataset)\n",
        "\n",
        "  comments = data['comments']\n",
        "\n",
        "  processed_article = process_text3(article)\n",
        "  all_bigrams = list(bigrams(processed_article))\n",
        "\n",
        "  len_ratios_tc = []\n",
        "  latency_ratios = []\n",
        "  npmi_list =[]\n",
        "  for comment,topic in zip(comments,topics):\n",
        "\n",
        "\n",
        "    # Size Reduction eval\n",
        "    comment_words_len = len(comment.split(' '))\n",
        "    topic_words_len = len(topic.split(' '))\n",
        "    len_ratios_tc.append(topic_words_len/comment_words_len)\n",
        "\n",
        "    # Latency eval\n",
        "    comment_words = comment.split(' ')\n",
        "    topic_words = topic.split(' ')\n",
        "    tw_appear = 0\n",
        "    for topic_word in topic_words:\n",
        "      tw_appear += int(topic_word in comment_words)\n",
        "    latency_ratio = tw_appear/topic_words_len\n",
        "    latency_ratios.append(latency_ratio)\n",
        "\n",
        "    # Relatedness to article eval using npmi\n",
        "    npmi_t = npmi_from_bigrams(topic=topic,all_bigrams=all_bigrams)\n",
        "    npmi_list.append(npmi_t)\n",
        "\n",
        "  # Taking average for the eval measures\n",
        "  len_ratio_avg = sum(len_ratios_tc)/len(len_ratios_tc)\n",
        "  latency_ratio_avg = sum(latency_ratios)/len(latency_ratios)\n",
        "  if len(npmi_list):\n",
        "    npmi_avg = sum(npmi_list)/len(npmi_list)\n",
        "  else:\n",
        "    npmi_avg = 0\n",
        "\n",
        "  # Store results in a dictionary\n",
        "  results = {\n",
        "      \"len_ratio_avg\": len_ratio_avg,\n",
        "      \"latency_ratio_avg\":latency_ratio_avg,\n",
        "      \"npmi_avg\":npmi_avg\n",
        "  }\n",
        "\n",
        "  with open('[RESULTS_NP]'+text_filename.replace('txt','json'),'w',encoding='utf-8') as output_file:\n",
        "    output_file.write(json.dumps(results,indent=4))"
      ],
      "metadata": {
        "id": "8iZsMRHrKbAo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "for text_file in text_files[1:]:\n",
        "  s = time.time()\n",
        "  eval_noun_phrase(text_file)\n",
        "  print(text_file,(time.time() - s) * 1e3,'ms')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        },
        "id": "2D_l4ok4-cPG",
        "outputId": "9603730b-d2e7-4a64-84fe-176dad7dcce9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1,374 Days_ My Life With Long Covid.txt 85070.57905197144 ms\n",
            "A Year on Ozempic Taught Me We’re Thinking About Obesity All Wrong.txt 128824.21660423279 ms\n",
            "Advice From a Psychotherapist on How to Cope Today.txt 8946.769952774048 ms\n",
            "Alzheimer’s Can Be a World of Endless Second Chances.txt 7657.370328903198 ms\n",
            "Are Smartphones Driving Our Teens to Depression_.txt 52677.24823951721 ms\n",
            "Are We Thinking About Obesity All Wrong_.txt 226161.64565086365 ms\n",
            "Deep Inside Mountains, Work Is Getting Much More Dangerous.txt 14252.062559127808 ms\n",
            "Doctors Need a Better Way to Treat Patients Without Their Consent.txt 45884.09781455994 ms\n",
            "Does Gene Editing Have a Future in Reproductive Medicine_.txt 26966.88437461853 ms\n",
            "Finding Light in Winter.txt 13076.559066772461 ms\n",
            "How to Help Americans Eat Less Junk Food.txt 33583.510398864746 ms\n",
            "How Virtual Appointments Taught Me to Be a Better Doctor.txt 20600.692749023438 ms\n",
            "How Well Does Masking Work_ And Other Pandemic Questions We Need to Answer..txt 43526.01361274719 ms\n",
            "It’s Not Easy to Tell People You Have Cancer. As a Doctor, I See It All the Time..txt 23867.919206619263 ms\n",
            "It’s Not Your Imagination. Your Allergies Are Getting Worse..txt 13756.488800048828 ms\n",
            "Learning to Love My Father as His Mind Unraveled.txt 28691.4119720459 ms\n",
            "Let People Sell Their Kidneys. It Will Save Lives..txt 64993.59202384949 ms\n",
            "Light in the Shadow of a Diagnosis.txt 12433.456182479858 ms\n",
            "My Mom Had Alzheimer’s. Now I Do Too, but I Learned From Her Not to Despair..txt 18501.830339431763 ms\n",
            "My Son Has a Rare Syndrome. So I Turned to the Internet..txt 70032.0188999176 ms\n",
            "No One Told Me This Would Happen to My Body in My 40s.txt 17423.641681671143 ms\n",
            "Not Everything We Call Cancer Should Be Called Cancer.txt 76706.35056495667 ms\n",
            "Should Human Life Be Optimized_.txt 139412.85037994385 ms\n",
            "Surgeon General_ Why I’m Calling for a Warning Label on Social Media Platforms.txt 60426.64051055908 ms\n",
            "Teaching Patients How to Heal.txt 7258.3160400390625 ms\n",
            "The Decline in Geriatric Care Hurts Us All.txt 32811.53416633606 ms\n",
            "The Devastating Legacy of Lies in Alzheimer’s Science.txt 82127.92944908142 ms\n",
            "The Heat Wave Scenario That Keeps Climate Scientists Up at Night.txt 85234.53736305237 ms\n",
            "The Human Toll of Nuclear Testing.txt 87175.40979385376 ms\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/NP_TOPICS/[NP_TOPICS_LIST]The Monster Measles Outbreak in Europe Is a Warning.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4175290540.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0meval_noun_phrase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1e3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ms'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-931249503.py\u001b[0m in \u001b[0;36meval_noun_phrase\u001b[0;34m(text_filename)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meval_noun_phrase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mnp_topics_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/NP_TOPICS'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_topics_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/[NP_TOPICS_LIST]'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtext_filename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/NP_TOPICS/[NP_TOPICS_LIST]The Monster Measles Outbreak in Europe Is a Warning.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i,t in enumerate(text_files):\n",
        "  print(i,t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwvL0BsDFqdh",
        "outputId": "22fab130-3cfe-485e-eab3-ec5da5c4bb99"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 48 Million Americans Live With Addiction. Here’s How to Get Them Help That Works..txt\n",
            "1 1,374 Days_ My Life With Long Covid.txt\n",
            "2 A Year on Ozempic Taught Me We’re Thinking About Obesity All Wrong.txt\n",
            "3 Advice From a Psychotherapist on How to Cope Today.txt\n",
            "4 Alzheimer’s Can Be a World of Endless Second Chances.txt\n",
            "5 Are Smartphones Driving Our Teens to Depression_.txt\n",
            "6 Are We Thinking About Obesity All Wrong_.txt\n",
            "7 Deep Inside Mountains, Work Is Getting Much More Dangerous.txt\n",
            "8 Doctors Need a Better Way to Treat Patients Without Their Consent.txt\n",
            "9 Does Gene Editing Have a Future in Reproductive Medicine_.txt\n",
            "10 Finding Light in Winter.txt\n",
            "11 How to Help Americans Eat Less Junk Food.txt\n",
            "12 How Virtual Appointments Taught Me to Be a Better Doctor.txt\n",
            "13 How Well Does Masking Work_ And Other Pandemic Questions We Need to Answer..txt\n",
            "14 It’s Not Easy to Tell People You Have Cancer. As a Doctor, I See It All the Time..txt\n",
            "15 It’s Not Your Imagination. Your Allergies Are Getting Worse..txt\n",
            "16 Learning to Love My Father as His Mind Unraveled.txt\n",
            "17 Let People Sell Their Kidneys. It Will Save Lives..txt\n",
            "18 Light in the Shadow of a Diagnosis.txt\n",
            "19 My Mom Had Alzheimer’s. Now I Do Too, but I Learned From Her Not to Despair..txt\n",
            "20 My Son Has a Rare Syndrome. So I Turned to the Internet..txt\n",
            "21 No One Told Me This Would Happen to My Body in My 40s.txt\n",
            "22 Not Everything We Call Cancer Should Be Called Cancer.txt\n",
            "23 Should Human Life Be Optimized_.txt\n",
            "24 Surgeon General_ Why I’m Calling for a Warning Label on Social Media Platforms.txt\n",
            "25 Teaching Patients How to Heal.txt\n",
            "26 The Decline in Geriatric Care Hurts Us All.txt\n",
            "27 The Devastating Legacy of Lies in Alzheimer’s Science.txt\n",
            "28 The Heat Wave Scenario That Keeps Climate Scientists Up at Night.txt\n",
            "29 The Human Toll of Nuclear Testing.txt\n",
            "30 The Monster Measles Outbreak in Europe Is a Warning.txt\n",
            "31 The New Age of D.I.Y. Medicine.txt\n",
            "32 The New Alcohol Warning Is Not a Prescription.txt\n",
            "33 The Problem Is With Men’s Sperm.txt\n",
            "34 The Problem With Saying ‘Sex Assigned at Birth’.txt\n",
            "35 The Way You Build Muscle Is the Way You Build a Life.txt\n",
            "36 There’s a Better Way to Talk About Fluoride, Vaccines and Raw Milk.txt\n",
            "37 This Diet Buzzword Is Misleading.txt\n",
            "38 This Is What It Takes to Get an Abortion in America.txt\n",
            "39 We Now Have a Chance to Stop the Most Deadly Infectious Disease — if We Act.txt\n",
            "40 We’re Relearning What Pandemics Do to a Society.txt\n",
            "41 WeightWatchers Got One Thing Very Right.txt\n",
            "42 What Having a Brother With Down Syndrome Has Taught Me About Everyone Else.txt\n",
            "43 What We Lose When Pharmacists Are Forced to Act Like Cops.txt\n",
            "44 When There’s a Dearth of Good Information on Women’s Health, a Million Scams Bloom.txt\n",
            "45 Why Are So Many Young Adults Getting Cancer_.txt\n",
            "46 Why Ultraprocessed Foods Aren’t Always Bad.txt\n",
            "47 Your Brain Has Tricked You Into Thinking Everything Is Worse.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "for text_file in text_files[31:]:\n",
        "  s = time.time()\n",
        "  eval_noun_phrase(text_file)\n",
        "  print(text_file,'|',(time.time() - s) * 1e3,'ms')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "id": "bUIifMVEF2i_",
        "outputId": "e06110ab-e68a-451f-95d2-94c6e67e88c8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The New Age of D.I.Y. Medicine.txt | 28484.822273254395 ms\n",
            "The New Alcohol Warning Is Not a Prescription.txt | 31900.86030960083 ms\n",
            "The Problem Is With Men’s Sperm.txt | 32527.349710464478 ms\n",
            "The Problem With Saying ‘Sex Assigned at Birth’.txt | 75716.21489524841 ms\n",
            "The Way You Build Muscle Is the Way You Build a Life.txt | 3867.5150871276855 ms\n",
            "There’s a Better Way to Talk About Fluoride, Vaccines and Raw Milk.txt | 74556.82635307312 ms\n",
            "This Diet Buzzword Is Misleading.txt | 5069.25368309021 ms\n",
            "This Is What It Takes to Get an Abortion in America.txt | 13932.663679122925 ms\n",
            "We Now Have a Chance to Stop the Most Deadly Infectious Disease — if We Act.txt | 10099.504947662354 ms\n",
            "We’re Relearning What Pandemics Do to a Society.txt | 8928.40027809143 ms\n",
            "WeightWatchers Got One Thing Very Right.txt | 47820.4927444458 ms\n",
            "What Having a Brother With Down Syndrome Has Taught Me About Everyone Else.txt | 1648.813009262085 ms\n",
            "What We Lose When Pharmacists Are Forced to Act Like Cops.txt | 23281.232118606567 ms\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/NP_TOPICS/[NP_TOPICS_LIST]When There’s a Dearth of Good Information on Women’s Health, a Million Scams Bloom.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4012458046.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m31\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0meval_noun_phrase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1e3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ms'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-931249503.py\u001b[0m in \u001b[0;36meval_noun_phrase\u001b[0;34m(text_filename)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meval_noun_phrase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mnp_topics_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/NP_TOPICS'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_topics_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/[NP_TOPICS_LIST]'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtext_filename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/NP_TOPICS/[NP_TOPICS_LIST]When There’s a Dearth of Good Information on Women’s Health, a Million Scams Bloom.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "for text_file in text_files[45:]:\n",
        "  s = time.time()\n",
        "  eval_noun_phrase(text_file)\n",
        "  print(text_file,'|',(time.time() - s) * 1e3,'ms')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcFefhogHaSK",
        "outputId": "dd1f8f79-f80b-40a8-b0d1-0ea5ce010fd8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why Are So Many Young Adults Getting Cancer_.txt | 18174.074172973633 ms\n",
            "Why Ultraprocessed Foods Aren’t Always Bad.txt | 64793.638944625854 ms\n",
            "Your Brain Has Tricked You Into Thinking Everything Is Worse.txt | 60338.059425354004 ms\n"
          ]
        }
      ]
    }
  ]
}